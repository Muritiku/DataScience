{"nbformat_minor": 1, "cells": [{"source": "# Interactive news Analyzer\n## Advanced Data Science Capstone project\n### by Anton Dziavitsyn 2019\n\n## Part 1: Data extraction", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "### Stage 1: download RAW data from newsapi", "cell_type": "markdown", "metadata": {}}, {"source": "**SETUP stage**  \n*Create spark context connected to Cloudant database*", "cell_type": "markdown", "metadata": {}}, {"source": "# Cloudant credentials\ncloudant_credentials = {\n    \"host\": \"REMOVED ON PUBLISHING\",\n    \"custom_url\": \"REMOVED ON PUBLISHING\",\n    \"username\": \"REMOVED ON PUBLISHING\",\n    \"password\": \"REMOVED ON PUBLISHING\"\n}\n\n# Spark session with attached cloudant storage\nspark = SparkSession\\\n    .builder\\\n    .appName(\"Interactive news Analyzer\")\\\n    .config(\"cloudant.host\",cloudant_credentials['custom_url'].split('@')[1])\\\n    .config(\"cloudant.username\", cloudant_credentials['username'])\\\n    .config(\"cloudant.password\",cloudant_credentials['password'])\\\n    .config(\"createDBOnSave\",\"true\")\\\n    .config(\"jsonstore.rdd.partitions\", 1)\\\n    .getOrCreate()", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": 1}, {"source": "**DOWNLOAD headlines stage**  \n*Download data about news sources and headlines from news api, and create dataframes to save them to Cloudant DB*", "cell_type": "markdown", "metadata": {}}, {"source": "#inputs\nfrom pyspark.sql import Row\nimport requests\n\n# Key to news API\nNEWS_API_KEY = \"REMOVED ON PUBLISHING\"\n\n# Helpers\n\n# function to create DF from list of dicts\ndef create_DF(list_of_dicts):\n    return spark.createDataFrame(Row(**x) for x in list_of_dicts)\n\n# Get all news sources and categories for English language news\nsource_url = \"https://newsapi.org/v2/sources?language=en&apiKey={0}\".format(NEWS_API_KEY)\nresponse = requests.get(source_url).json()\n#create dataframe\ndf_raw_sources = create_DF(response['sources'])\ndf_raw_sources.cache()\n# Write news sources data to Cloudant DB\ndf_raw_sources.write.format(\"com.cloudant.spark\").save(\"raw_news_sources\")\n# Show info about data\nprint(\"News sources downloaded: {0}\".format(df_raw_sources.count()))\nprint(\"First 10 sources:\")\ndf_raw_sources.show(10)\n\n# Then we get TOP headline news for every source\n# function to get headlines by source\ndef get_headlines(source):\n    try:\n        source_url = \"https://newsapi.org/v2/top-headlines?sources={0}&apiKey={1}\".format(source.id, NEWS_API_KEY)\n        response = requests.get(source_url).json()\n        return response['articles']\n    except:\n        return None\n\n# map function to get list of headlines\nlist_headlines = []\nfor news in df_raw_sources.rdd.map(get_headlines).collect():\n    if news is not None:\n        list_headlines = list_headlines + news\n#create dataframe\ndf_raw_headlines = create_DF(list_headlines)\ndf_raw_headlines.cache()\n# Show info about data\nprint(\"News headlines downloaded: {0}\".format(df_raw_headlines.count()))\nprint(\"First 10 headlines:\")\ndf_raw_headlines.show(10)\n\n# Write headlines data to Cloudant DB\n#df_raw_headlines.write.format(\"com.cloudant.spark\").save(\"raw_news_headlines\")", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "News sources downloaded: 91\nFirst 10 sources:\n+----------+-------+--------------------+--------------------+--------+--------------------+--------------------+\n|  category|country|         description|                  id|language|                name|                 url|\n+----------+-------+--------------------+--------------------+--------+--------------------+--------------------+\n|   general|     us|Your trusted sour...|            abc-news|      en|            ABC News|https://abcnews.g...|\n|   general|     au|Australia's most ...|         abc-news-au|      en|       ABC News (AU)|http://www.abc.ne...|\n|   general|     us|News, analysis fr...|  al-jazeera-english|      en|  Al Jazeera English|http://www.aljaze...|\n|technology|     us|The PC enthusiast...|        ars-technica|      en|        Ars Technica|http://arstechnic...|\n|   general|     us|The AP delivers i...|    associated-press|      en|    Associated Press| https://apnews.com/|\n|  business|     au|The Australian Fi...|australian-financ...|      en|Australian Financ...|  http://www.afr.com|\n|   general|     us|Axios are a new m...|               axios|      en|               Axios|https://www.axios...|\n|   general|     gb|Use BBC News for ...|            bbc-news|      en|            BBC News|http://www.bbc.co...|\n|    sports|     gb|The home of BBC S...|           bbc-sport|      en|           BBC Sport|http://www.bbc.co...|\n|    sports|     us|Sports journalist...|     bleacher-report|      en|     Bleacher Report|http://www.bleach...|\n+----------+-------+--------------------+--------------------+--------+--------------------+--------------------+\nonly showing top 10 rows\n\nNews headlines downloaded: 854\nFirst 10 headlines:\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|              author|             content|         description|         publishedAt|              source|               title|                 url|          urlToImage|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    Lucien Bruggeman|President Donald ...|The longtime Trum...|2019-01-29T00:00:00Z|Map(name -> ABC N...|Roger Stone expec...|https://abcnews.g...|https://s.abcnews...|\n|The Associated Press|California regula...|PG&E faces billio...|2019-01-29T00:00:00Z|Map(name -> ABC N...|California regula...|https://abcnews.g...|https://s.abcnews...|\n|             ABCNews|Acting Attorney G...|Acting Attorney G...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Acting attorney g...|https://abcnews.g...|https://s.abcnews...|\n|             ABCNews|Michael Cohen has...|Michael Cohen wil...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Michael Cohen to ...|https://abcnews.g...|https://s.abcnews...|\n|      Conor Finnegan|The Trump adminis...|Venezuela's Nicol...|2019-01-28T00:00:00Z|Map(name -> ABC N...|US sanctions Vene...|https://abcnews.g...|https://s.abcnews...|\n|      John Parkinson|President Donald ...|President Donald ...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Trump accepts Pel...|https://abcnews.g...|https://s.abcnews...|\n|           Luke Barr|A 13-count indict...|The U.S. on Monda...|2019-01-28T00:00:00Z|Map(name -> ABC N...|US levels crimina...|https://abcnews.g...|https://s.abcnews...|\n|      Aaron Katersky|The first crimina...|John Kapoor has p...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Trial begins for ...|https://abcnews.g...|https://s.abcnews...|\n|Luis Martinez and...|Top U.S. and NATO...|Top U.S. and NATO...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Pentagon and NATO...|https://abcnews.g...|https://s.abcnews...|\n|      Tessa Weinberg|The nonpartisan C...|The nonpartisan C...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Shutdown cost eco...|https://abcnews.g...|https://s.abcnews...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"}], "execution_count": 2}, {"source": "**Check RAW data from newsapi**", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.sql.functions import countDistinct\n\n# get distinct autors from raw headlines data\nprint(\"Distinct authors:\")\ndf_raw_headlines.agg(countDistinct(\"author\")).show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Distinct authors:\n+----------------------+\n|count(DISTINCT author)|\n+----------------------+\n|                   464|\n+----------------------+\n\n"}], "execution_count": 3}, {"source": "**Conclusions:**\n+ We have headlines collection, with descriptions and links to articles\n+ We have sources collection with info about publishers", "cell_type": "markdown", "metadata": {}}, {"source": "### Stage 2: download articles HTML and extract publication text", "cell_type": "markdown", "metadata": {}}, {"source": "**DOWNLOAD and CLEAN articles data**", "cell_type": "markdown", "metadata": {}}, {"source": "# imports\nfrom bs4 import BeautifulSoup\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\n\n# remove duplicate lines and return Clean Text as list of lines\ndef remove_dupe_lines(text):\n    lines_seen = set()\n    result = []\n    for line in text.split('\\n'):\n        if line not in lines_seen:\n            result.append(line)\n            lines_seen.add(line)\n    return result\n\n# function intersect 2 arrays (lists) and return intersection result\ndef textIntersection(arr1, arr2):\n    return list(filter(lambda x: x in arr1, arr2))\n\n# get and clean article text\ndef get_article_text(article_url):\n    try:\n        html = requests.get(article_url).text\n        soup = BeautifulSoup(html, \"lxml\")\n\n        # kill all script and style elements\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        # get text\n        text = soup.get_text()\n\n        # break into lines and remove leading and trailing space on each\n        lines = (line.strip() for line in text.splitlines())\n        # break multi-headlines into a line each\n        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n        # drop blank lines\n        text = '\\n'.join(chunk for chunk in chunks if chunk)\n        # remove duplication lines\n        text_lines = remove_dupe_lines(text)\n        # remove linebreaks in lines\n        text_lines = [line.replace('/n', ' ').replace('/r', ' ').replace('  ', ' ') for line in text_lines]\n        \n        return text_lines\n    except:\n        return []\n\n# Download and clean articles HTML to get clear text (list of lines)\nudfGetArticleText = udf(get_article_text, ArrayType(StringType()))\ndf_raw_articles = df_raw_headlines.withColumn(\"article\", udfGetArticleText(\"url\")).filter(F.size(F.col(\"article\")) > 0)\ndf_raw_articles.cache()\n\n# Write articles data to Cloudant DB\n#df_raw_articles.write.format(\"com.cloudant.spark\").save(\"raw_headlines_with_articles\")\n\n# Show info about data\nprint(\"News articles downloaded: {0}\".format(df_raw_articles.count()))\nprint(\"First 10 articles records:\")\ndf_raw_articles.show(10)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "News articles downloaded: 852\nFirst 10 articles records:\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|              author|             content|         description|         publishedAt|              source|               title|                 url|          urlToImage|             article|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    Lucien Bruggeman|President Donald ...|The longtime Trum...|2019-01-29T00:00:00Z|Map(name -> ABC N...|Roger Stone expec...|https://abcnews.g...|https://s.abcnews...|[Roger Stone expe...|\n|The Associated Press|California regula...|PG&E faces billio...|2019-01-29T00:00:00Z|Map(name -> ABC N...|California regula...|https://abcnews.g...|https://s.abcnews...|[PG&E files for b...|\n|             ABCNews|Acting Attorney G...|Acting Attorney G...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Acting attorney g...|https://abcnews.g...|https://s.abcnews...|[Acting Attorney ...|\n|             ABCNews|Michael Cohen has...|Michael Cohen wil...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Michael Cohen to ...|https://abcnews.g...|https://s.abcnews...|[Michael Cohen to...|\n|      Conor Finnegan|The Trump adminis...|Venezuela's Nicol...|2019-01-28T00:00:00Z|Map(name -> ABC N...|US sanctions Vene...|https://abcnews.g...|https://s.abcnews...|[Venezuela's Madu...|\n|      John Parkinson|President Donald ...|President Donald ...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Trump accepts Pel...|https://abcnews.g...|https://s.abcnews...|[Pelosi invites T...|\n|           Luke Barr|A 13-count indict...|The U.S. on Monda...|2019-01-28T00:00:00Z|Map(name -> ABC N...|US levels crimina...|https://abcnews.g...|https://s.abcnews...|[US levels crimin...|\n|      Aaron Katersky|The first crimina...|John Kapoor has p...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Trial begins for ...|https://abcnews.g...|https://s.abcnews...|[Trial begins for...|\n|Luis Martinez and...|Top U.S. and NATO...|Top U.S. and NATO...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Pentagon and NATO...|https://abcnews.g...|https://s.abcnews...|[Pentagon and NAT...|\n|      Tessa Weinberg|The nonpartisan C...|The nonpartisan C...|2019-01-28T00:00:00Z|Map(name -> ABC N...|Shutdown cost eco...|https://abcnews.g...|https://s.abcnews...|[Report estimates...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"}], "execution_count": 4}, {"source": "**Removing site related text**  \n*Now we have the articles text, but this text contains site related text (headers, buttons etc).*  \n*We need to intersect articles lines from one site to find common site text, and remove it from article. (that is why I store articles as lines collection)*", "cell_type": "markdown", "metadata": {}}, {"source": "from functools import reduce\n\n# function for List intersection\ndef intersection(lst1, lst2): \n    lst3 = [value for value in lst1 if value in lst2] \n    return lst3\n\n# function for intersect all articles in list\ndef intersect_lists(lst):\n    if len(lst) > 1:\n        return reduce((lambda x, y: intersection(x, y)), lst)\n    else:\n        return [[]]\n\nudfItersectLists = udf(intersect_lists, ArrayType(StringType()))\n# intersect all articles with groupping by article source\ndf_site_related = df_raw_articles.groupBy('source.id').agg(F.collect_list('article').alias('site_lines')).withColumn('site_lines', udfItersectLists('site_lines'))\ndf_site_related.cache()\n\n#function to return clean article text (with rows which are not in site data)\n# returns clean text without line breaks\ndef get_clean_article(article, site_data):\n    return ' '.join(row for row in article if row not in set(site_data))\n\nudfGetCleanText = udf(get_clean_article, StringType())\n\ndf_clean_articles = df_raw_articles.join(df_site_related, df_raw_articles.source.id == df_site_related.id, how='left').withColumn('article', udfGetCleanText('article', 'site_lines')).drop('site_lines', 'df_site_related.id')\ndf_clean_articles.cache()\n# Write articles data to Cloudant DB\ndf_clean_articles.write.format(\"com.cloudant.spark\").save(\"clean_articles\")\n\n# Show info about data\nprint(\"Clean articles: {0}\".format(df_clean_articles.count()))\nprint(\"First 10 articles records:\")\ndf_clean_articles.show(10)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Clean articles: 852\nFirst 10 articles records:\n+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|          author|             content|         description|         publishedAt|              source|               title|                 url|          urlToImage|             article|                  id|\n+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n| Katherine Blunt|PG&amp;E Corp. fi...|California utilit...|2019-01-29T08:22:00Z|Map(name -> The W...|PG&E Files for Ba...|https://www.wsj.c...|https://images.ws...|PG&E Files for Ba...|the-wall-street-j...|\n| Robert McMillan|Apple Inc. scramb...|Apple scrambled t...|2019-01-29T05:50:00Z|Map(name -> The W...|Apple Bug Enables...|https://www.wsj.c...|https://images.ws...|Apple Bug Enables...|the-wall-street-j...|\n|       Bob Davis|WASHINGTONCabinet...|Cabinet-level del...|2019-01-28T23:19:00Z|Map(name -> The W...|Big Divides Remai...|https://www.wsj.c...|https://images.ws...|Big Divides Remai...|the-wall-street-j...|\n| Natalie Andrews|WASHINGTONPreside...|President Trump a...|2019-01-28T21:43:00Z|Map(name -> The W...|Trump Accepts New...|https://www.wsj.c...|https://images.ws...|Trump Accepts New...|the-wall-street-j...|\n|    Sadie Gurman|WASHINGTONSpecial...|Acting U.S. Attor...|2019-01-28T21:42:00Z|Map(name -> The W...|Interim Attorney ...|https://www.wsj.c...|https://images.ws...|Interim Attorney ...|the-wall-street-j...|\n|   Kate O\u2019Keeffe|The Trump adminis...|The Trump adminis...|2019-01-28T21:34:00Z|Map(name -> The W...|U.S. Authorities ...|https://www.wsj.c...|https://images.ws...|U.S. Authorities ...|the-wall-street-j...|\n|Alexandra Berzon|Wynn Resorts Ltd....|Wynn Resorts exec...|2019-01-28T21:30:00Z|Map(name -> The W...|Nevada: Wynn Reso...|https://www.wsj.c...|https://images.ws...|Nevada: Wynn Reso...|the-wall-street-j...|\n|      Ian Talley|The U.S. imposed ...|The U.S. imposed ...|2019-01-28T20:43:00Z|Map(name -> The W...|U.S. Imposes Sanc...|https://www.wsj.c...|https://images.ws...|U.S. Imposes Sanc...|the-wall-street-j...|\n|Louise Radnofsky|WASHINGTONThe imm...|The immigration-c...|2019-01-28T19:07:00Z|Map(name -> The W...|Shutdown Compound...|https://www.wsj.c...|https://images.ws...|Shutdown Compound...|the-wall-street-j...|\n|       Asa Fitch|Nvidia Corp. lowe...|Nvidia lowered it...|2019-01-28T15:53:00Z|Map(name -> The W...|Nvidia Lowers Gui...|https://www.wsj.c...|https://images.ws...|Nvidia Lowers Gui...|the-wall-street-j...|\n+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"}], "execution_count": 5}, {"source": "**Let us look into one article text - it is clean enough for processing**", "cell_type": "markdown", "metadata": {}}, {"source": "df_clean_articles.limit(1)[['article']].collect()[0][0]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "\"PG&E Files for Bankruptcy Following California Wildfires - WSJ Dow Jones, a News Corp companyNews Corp is a network of leading companies in the worlds of diversified media, news, education, and information servicesDow JonesBarron'sBigChartsDJXDow Jones NewswiresFactivaFinancial NewsMansion GlobalMarketWatchNewsPlusPrivate MarketsRisk & ComplianceWSJ ConferenceWSJ ProWSJ VideoWSJ.comNews CorpBig DecisionsBusiness SpectatorCheckout51Harper CollinsHousingMakaanNew York PostNews America MarketingPropTigerREArealtor.comStoryfulThe AustralianThe SunThe TimesUnrulySubscribeSign InThe Wall Street JournalEurope EditionU.S.AsiaEuropeIndia\u4e2d\u56fd (China)\u65e5\u672c (Japan)January 28, 2019Print EditionVideoHomeWorldRegionsAfricaAsiaCanadaChinaEuropeLatin AmericaMiddle EastSectionsEconomyMoreWorld VideoU.S.SectionsEconomyLawNew YorkPoliticsColumns & BlogsReal Time EconomicsWashington WireMoreJournal ReportU.S. VideoWhat's News PodcastPoliticsBlogsWashington WireMorePolitics VideoWSJ/NBC News PollEconomyBlogsReal Time EconomicsWSJ ProBankruptcyCentral BankingPrivate EquityVenture CapitalMoreEconomic Forecasting SurveyEconomy VideoBusinessSectionsManagementTech/WSJ.DThe Future of EverythingIndustriesAerospace & DefenseAutos & TransportationCommercial Real EstateConsumer ProductsEnergyEntrepreneurshipFinancial ServicesFood & ServicesHealth CareHospitalityLawManufacturingMedia & MarketingNatural ResourcesRetailC-SuiteCFO JournalCIO JournalCMO TodayLogistics ReportRisk & ComplianceColumnsHeard on the StreetWSJ ProArtificial IntelligenceBankruptcyCentral BankingPrivate EquityVenture CapitalMoreBusiness VideoJournal ReportBusiness PodcastTechSectionsCIO JournalThe Future of EverythingColumns & BlogsChristopher MimsJoanna SternDavid PierceMoreBillion Dollar Startup ClubTech VideoTech PodcastStartup Stock TrackerMarketsSectionsBondsCommercial Real EstateCommodities & FuturesStocksYour MoneyColumns & BlogsHeard on the StreetMoneyBeatWealth AdviserAhead of the TapeMarket DataMarket Data HomeU.S. StocksCurrenciesCommoditiesBonds & RatesMoreCFO JournalJournal ReportMarkets VideoYour Money Briefing PodcastSecrets of Wealthy Women PodcastSearch Quotes and CompaniesOpinionColumnistsSadanand DhumeJames FreemanWilliam A. GalstonDaniel HenningerHolman W. JenkinsAndy KesslerWilliam McGurnWalter Russell MeadPeggy NoonanMary Anastasia O'GradyJason RileyJoseph SternbergKimberley A. StrasselReviewsBooksFilmTelevisionTheaterArtMasterpiece SeriesMusicDanceOperaExhibitionCultural CommentaryMoreEditorialsCommentaryLetters to the EditorThe Weekend InterviewPotomac Watch PodcastForeign Edition PodcastOpinion VideoNotable & QuotableBest of the Web NewsletterMorning Editorial Report NewsletterLife & ArtsSectionsArtsBooksCarsFood & DrinkHealthIdeasReal EstateScienceSportsStyle & FashionTravelMoreWSJ. MagazineWSJ PuzzlesThe Future of EverythingFar & AwayLife VideoArts VideoReal EstateSectionsCommercial Real EstateHouse of the DayMoreReal Estate VideoWSJ. MagazineSectionsFashionArt & DesignTravelFoodCultureSearchSearch\u2192 Business PG&E Files for Bankruptcy Following California Wildfires Utility seeks chapter 11 protection Katherine Blunt and Russell Gold BiographyRussell Gold @russellgold Google+ Russell.Gold@wsj.com Updated Jan. 29, 2019 3:40 a.m. ET PG&E Corp. filed for bankruptcy protection on Tuesday as it struggles with billions of dollars in potential liabilities from its role in sparking California wildfires, triggering one of the most complex corporate reorganization cases in years. California\u2019s largest utility, which provides natural gas and electric service to 16 million people, sought protection under chapter 11 of the bankruptcy code. The process to restructure its debts is expected to be protracted, involving state and federal regulators, with wide-ranging...\""}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "**Now we have: Headline articles collection with clean article text, and info about publisher**\n\n## END of Part 1: Data extraction\n### by Anton Dziavitsyn 2019", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark 2.1", "name": "python3-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}